{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[0.00966449]\n",
      " [0.00786506]\n",
      " [0.99358898]\n",
      " [0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.47626534967276324\n",
      "Loss at epoch 1000: 0.13489859988457437\n",
      "Loss at epoch 2000: 0.11336980911609598\n",
      "Loss at epoch 3000: 0.10160842764304641\n",
      "Loss at epoch 4000: 0.09351230793315884\n",
      "Loss at epoch 5000: 0.08784077111262008\n",
      "Loss at epoch 6000: 0.08356987981592819\n",
      "Loss at epoch 7000: 0.08017362026415693\n",
      "Loss at epoch 8000: 0.07737462416349243\n",
      "Loss at epoch 9000: 0.07500877307965147\n",
      "Loss at epoch 10000: 0.07296931516994946\n",
      "Loss at epoch 11000: 0.07118255275734814\n",
      "Loss at epoch 12000: 0.06959578006806799\n",
      "Loss at epoch 13000: 0.06817044921959414\n",
      "Loss at epoch 14000: 0.06687789291035288\n",
      "Loss at epoch 15000: 0.06569647449219508\n",
      "Loss at epoch 16000: 0.06460962091366385\n",
      "Loss at epoch 17000: 0.06360443945374648\n",
      "Loss at epoch 18000: 0.06267073537254166\n",
      "Loss at epoch 19000: 0.06180031012925233\n",
      "Loss at epoch 20000: 0.06098645759890478\n",
      "Loss at epoch 21000: 0.060223600555038524\n",
      "Loss at epoch 22000: 0.0595070267933815\n",
      "Loss at epoch 23000: 0.05883269627422701\n",
      "Loss at epoch 24000: 0.05819709910588254\n",
      "Loss at epoch 25000: 0.0575971501156418\n",
      "Loss at epoch 26000: 0.05703010989870368\n",
      "Loss at epoch 27000: 0.056493525134024815\n",
      "Loss at epoch 28000: 0.05598518299055006\n",
      "Loss at epoch 29000: 0.055503075884621174\n",
      "Loss at epoch 30000: 0.05504537387341878\n",
      "Loss at epoch 31000: 0.05461040270541177\n",
      "Loss at epoch 32000: 0.054196626082130925\n",
      "Loss at epoch 33000: 0.053802631074370784\n",
      "Loss at epoch 34000: 0.05342711592036935\n",
      "Loss at epoch 35000: 0.05306887964186802\n",
      "Loss at epoch 36000: 0.05272681306639018\n",
      "Loss at epoch 37000: 0.052399890955266495\n",
      "Loss at epoch 38000: 0.05208716501767128\n",
      "Loss at epoch 39000: 0.051787757649205485\n",
      "Loss at epoch 40000: 0.05150085627536353\n",
      "Loss at epoch 41000: 0.05122570820998002\n",
      "Loss at epoch 42000: 0.050961615959771396\n",
      "Loss at epoch 43000: 0.05070793292081156\n",
      "Loss at epoch 44000: 0.05046405942297227\n",
      "Loss at epoch 45000: 0.050229439085319204\n",
      "Loss at epoch 46000: 0.0500035554500982\n",
      "Loss at epoch 47000: 0.04978592886593462\n",
      "Loss at epoch 48000: 0.0495761135926682\n",
      "Loss at epoch 49000: 0.04937369510122192\n",
      "Loss at epoch 50000: 0.04917828754234666\n",
      "Loss at epoch 51000: 0.04898953135830826\n",
      "Loss at epoch 52000: 0.04880709101197365\n",
      "Loss at epoch 53000: 0.04863065280884262\n",
      "Loss at epoch 54000: 0.04845992279008573\n",
      "Loss at epoch 55000: 0.048294624679626516\n",
      "Loss at epoch 56000: 0.048134497876948325\n",
      "Loss at epoch 57000: 0.04797929550094855\n",
      "Loss at epoch 58000: 0.04782878250972063\n",
      "Loss at epoch 59000: 0.04768273394642182\n",
      "Loss at epoch 60000: 0.047540933389900765\n",
      "Loss at epoch 61000: 0.04740317171460357\n",
      "Loss at epoch 62000: 0.047269246277612906\n",
      "Loss at epoch 63000: 0.0471389606391062\n",
      "Loss at epoch 64000: 0.04701212487544462\n",
      "Loss at epoch 65000: 0.04688855645960157\n",
      "Loss at epoch 66000: 0.04676808157520149\n",
      "Loss at epoch 67000: 0.04665053662794978\n",
      "Loss at epoch 68000: 0.046535769659055023\n",
      "Loss at epoch 69000: 0.046423641376935415\n",
      "Loss at epoch 70000: 0.04631402560775583\n",
      "Loss at epoch 71000: 0.04620680909463149\n",
      "Loss at epoch 72000: 0.046101890706155765\n",
      "Loss at epoch 73000: 0.0459991802088211\n",
      "Loss at epoch 74000: 0.045898596797119426\n",
      "Loss at epoch 75000: 0.04580006756439073\n",
      "Loss at epoch 76000: 0.045703526055425533\n",
      "Loss at epoch 77000: 0.04560891098914578\n",
      "Loss at epoch 78000: 0.045516165191671454\n",
      "Loss at epoch 79000: 0.045425234744225766\n",
      "Loss at epoch 80000: 0.045336068327964635\n",
      "Loss at epoch 81000: 0.04524861673673364\n",
      "Loss at epoch 82000: 0.045162832525604116\n",
      "Loss at epoch 83000: 0.04507866976465393\n",
      "Loss at epoch 84000: 0.044996083871449315\n",
      "Loss at epoch 85000: 0.044915031500457225\n",
      "Loss at epoch 86000: 0.04483547047229024\n",
      "Loss at epoch 87000: 0.044757359729799315\n",
      "Loss at epoch 88000: 0.04468065931143132\n",
      "Loss at epoch 89000: 0.04460533033497589\n",
      "Loss at epoch 90000: 0.04453133498689476\n",
      "Loss at epoch 91000: 0.044458636513979806\n",
      "Loss at epoch 92000: 0.04438719921523016\n",
      "Loss at epoch 93000: 0.04431698843264386\n",
      "Loss at epoch 94000: 0.04424797054020442\n",
      "Loss at epoch 95000: 0.04418011293072734\n",
      "Loss at epoch 96000: 0.044113384000497156\n",
      "Loss at epoch 97000: 0.04404775313179196\n",
      "Loss at epoch 98000: 0.04398319067349266\n",
      "Loss at epoch 99000: 0.04391966792002986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.66666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the Iris dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Normalize the features\n",
    "X_normalized = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the sigmoid function and its derivative\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights randomly with mean 0\n",
    "input_size = 4\n",
    "hidden_size = 5\n",
    "output_size = 3  # Three classes for the Iris dataset\n",
    "\n",
    "# Initialize the weights\n",
    "syn0 = 2 * np.random.random((input_size, hidden_size)) - 1  # First layer of weights\n",
    "syn1 = 2 * np.random.random((hidden_size, output_size)) - 1  # Second layer of weights\n",
    "\n",
    "# Training parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "num_epochs = 100000\n",
    "\n",
    "# Training the neural network\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    l0 = X_train\n",
    "    l1 = sigmoid(np.dot(l0, syn0))\n",
    "    l2 = sigmoid(np.dot(l1, syn1))\n",
    "    \n",
    "    # Compute the error\n",
    "    l2_loss = np.eye(output_size)[y_train] - l2\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        # Print the mean loss at every 1000 epochs\n",
    "        print(f\"Loss at epoch {epoch}: {np.mean(np.abs(l2_loss))}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    l2_delta = l2_loss * sigmoid(l2, deriv=True)\n",
    "    l1_loss = l2_delta.dot(syn1.T)\n",
    "    l1_delta = l1_loss * sigmoid(l1, deriv=True)\n",
    "    \n",
    "    # Update weights\n",
    "    syn1 += alpha * l1.T.dot(l2_delta)\n",
    "    syn0 += alpha * l0.T.dot(l1_delta)\n",
    "\n",
    "# Test the model on the training set (for demonstration purposes; in practice, you'd also want to test on a separate test set)\n",
    "l1 = sigmoid(np.dot(X_test, syn0))\n",
    "l2_train = sigmoid(np.dot(l1, syn1))\n",
    "predictions_train = np.argmax(l2_train, axis=1)\n",
    "\n",
    "accuracy_train = np.mean(predictions_train == y_test) * 100\n",
    "accuracy_train\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
